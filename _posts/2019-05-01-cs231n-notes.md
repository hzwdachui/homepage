---
title: 'CV notes'
date: 2019-05-01
permalink: /posts/2019/09/CV-notes/
tags:
  - Course Notes
---

# Intro ML notes
- hyperparameters
- two central approaches to statistics: frequentist estimators and Bayesian inference.
- Most machine learning algorithms can be divided into the categories of supervisedlearning and unsupervised learning
- stochastic gradient stochastic gradient

Learning Algorithms

The Task, T  
Machine learning tasks are usually described in terms of how the machinelearning system should process anexample. An example is a collection offeaturesthat have been quantitatively measured from some object or event that we wantthe machine learning system to process.  
Classiﬁcation    
Classiﬁcation with missing inputs:  
Regression  
Transcription  
Machine translation  
Structured output

The Performance Measure, P  
accuracy: the proportion of examples for which the model produces the correct output.  
 theerror rate, the proportionof examples for which the model produces an incorrect output  
 the average log-probability
 
 The Experience, E
 
 Capacity, Overﬁtting and Underﬁtting  
generalization  generalization error
data-generating process.    i.i.d. assumptions

The factors determining how well a machinelearning algorithm will perform are its ability to
1. Make the training error small.
2. Make the gap between training and test error small

underﬁtting and overﬁtting

capacity  a model’s capacity is its ability to ﬁt a wide variety offunctions

no freelunch theorem

# KNN and Linear Classifier
- image classifier: nearest neighbor classifier; KNN
- linear classifier(之后的FC layer就是)

**image classifier:** 
- Difference between L1 and L2 distance: 
L2 punishes differences between two vectors much more heavily
- nearest neighbor classifier is linear(1-nn)
	- no time at all to train, but very expensive to test
- k-nearest neighbor classifier: 
	- Train error of 1-nn will always be zero, test error of a 1-nn may be better or worse than a 5-nn
	- the decision boundary of a KNN classifier is **non-linear**.
	- higher values of k in the KNN classifier have what effect: smoothing effect that is more resistant to outliers(大概就是更光滑的意思吧)

**Linear classifier:** 
- W(C, D) ; X(N, D)
- each row of W corresponds to a "**template**" (or prototype) for each of the classes
- bias:  bias is what enables translations away from the origin
	- bias trick: Extend every vector x such that it always holds the constant 1 - a default bias dimension. Now the new score function simplifies to f(x) = Wx

# Loss and Optimization
- L1
- L2
- SVM
- softmax
- hinge loss 

**loss function** is a measure of unhappiness with our current model
**cross-validation：** iterate over different validation sets and average performance across them

## softmax
**cross entropy loss：** 
with **softmax**, we replace hinge loss with **cross entropy loss**

**numeric stability trick for softmax：**
Multiplying C to both denominator and numerator 

**what is the "probability" in softmax：**
How peaky or diffuse these probabilities are depends directly on the regularization strength λ - which you are in charge of as input to the system. Hence, the probabilities computed by the Softmax classifier are better thought of as **confidences** where, similar to the SVM, **the ordering of the scores is interpretable**, but the absolute numbers (or their differences) technically are not.

Compared to the softmax classifier, the SVM classifier has a more **local** objective([ref](http://cs231n.github.io/linear-classify/#svmvssoftmax))
意思就是SVM只会关注target的分数和pred的分数，不会管其他的，而softmax会把所有的得分都计算在loss中。

**compare centered difference and non-centered:** （重要：在activation和Data processing中都会遵循这个）
- different formula for numerical gradients: 以后补latex(centered反而可能会更不好XD)
	- centered numerical gradient **less good at** avoiding kinks：Coarser (larger) stepsizes always result in less accurate numerical approximations
- **problem of not centered**: Because if the data coming into a neuron is always positive, then the gradients on all the weights w will during backpropagation be all positive or all negative, creating a zig-zag dynamic in the gradient updates for the weights

什么情况下会不能通过**gradient checking**？
- Nonlinearity could contribute to error in gradient checking(SVM, Maxpooling) 原因是：numerical analysis could be different to Analytical derivative
- 例子：Consider gradient checking the ReLU function at x=-1e6. The gradient here is zero, but a numerical gradient might compute a non-zero gradient because f(x+h) might cross over the kink and introduce a non-zero contribution.
- we should first turn off regularization and check data loss alone first: regularization loss may overwhelm data loss and mask an incorrect implementation of data loss

**loss of softmax**
will be -log(1/N) at the very beginning (useful for sanity check)

## L1 L2 loss
- why is L2 loss harder to optimize than a stable loss like Softmax? 
it requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations(说白了就是太sentitive，不容易调参数)
- why is l2 loss less robust?
outliers can introduce huge gradients （意思就是对偏离的点，L2更不友好，L1包容性更好）

## loss and learning rate
![](https://o.quizlet.com/EZ0cihriWRVUyPaS8puxBg.png)
all learning rate is useful, update it all the time
**annealing learning rate over time**
Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function.
e.g. step decay, exponential decay, 1/t decay, cosine

## regularization penalty during optimization
with a regularization penalty, we can never achieve a loss of 0 (only occurs in pathological setting of W=0)

- L1
- L2

comparing L1 and L2 regularization
- L1 is more likely to sparse
- L2 is smaller and more diffuse

> [difference between L1,L2 loss and L1,L2 regularization](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)
> L1,L2 loss 是计算loss target和pred的距离和或距离平方和； L1，L2 regularization是一种附加在data loss上的reg loss，分别是W和W的平方和

# Train Neural Networks
主要任务
- 有哪些层，作用分别是什么
- 每种算法的优缺点是什么，适合在什么场景下使用？
- 训练一个模型的步骤是什么？
- Improve your training error:
	- Optimizers
	- Learning rate schedules
- Improve your test error:
	- Regularization
	- Choosing Hyperparameters
- Transfer Learning: Use less data

## 训练的步骤
1. One time setup
activation functions, preprocessing, weight initialization, regularization, gradient checking
2. Training dynamics
babysitting the learning process,
parameter updates, hyperparameter optimization
3. Evaluation
model ensembles, test-time augmentation

> activation functions: 一个非线性的函数，作用是：？？？
> regularization: inprove **single model** performance; 主要是为了训练没那么好，避免overfitting
> **Model Ensemble:** 1. Train multiple independent models 2. At test time average their results


## Vanishing/Exploding Gradient
Vanishing Gradient:
- Gradient becomes too small
- Some causes:
	- Choice of activation function
	- Multiplying many small numbers together
	- improper weight initialization

Exploding Gradient:
- Gradient becomes too large

## 每层介绍
### Activation Function
- Sigmoid [0,1]
- tanh [-1,1]
- ReLU [0,1]

#### Sigmoid
**Sigmoid的缺点：**
- Saturated neurons “kill” the gradients (有些部分斜率太平了)
-  Sigmoid outputs are not zero-centered 
	- Because if the data coming into a neuron is always positive, then the gradients on all the weights w will during backpropagation be all positive or all negative, creating a zig-zag dynamic in the gradient updates for the weights

> **为什么需要zero centered？**
> 假设用Sigmoid
> $$f\(WX + b\) \> 0 (Sigmoid会造成全部>0)$$ 
> $${f}'\left ( WX+b \right ) \> 0(Sigmoid的性质)$$
> $$\frac{\partial L}{\partial W} = {f}'(WX+b) * x (符号取决于x的大小)$$
> 所以input需要zero centered

#### tanh(x)
- Squashes numbers to range [-1,1]
- zero centered (nice)
- still kills gradients when saturated (斜率太平)

#### ReLU
**ReLU的优点：**
- Does not saturate (in +region) （一直是1）
- Very computationally efficient （max(x, 0)）
- **Converges much faster** than sigmoid/tanh in practice

**ReLU的缺点：**
- not **zero-centered** output
- dead relu problem: slope = 0 (x<0)
	- initialize it with a small bias
	- or use **leaky ReLU**

**Maxout “Neuron”**
- Does not have the basic form of dot product -> nonlinearity
- Generalizes ReLU and Leaky ReLU
- Linear Regime! Does not saturate! Does not die!
**Problem:** doubles the number of parameters/neuron :(

**Dead ReLU**
- poor weight initialization; too high learning rate
- consider the case where your affine layer results in an output vector of all zeros, so the relu function is all zeros as well. then d(relu)/d(affine) = 0, and your dL/d(affine)=0 as well, so no update gets performed

### Data Processing
we need **zero-centered** data and **normalization**

**优点：less sensitive to small changes in weights; easier to optimize**

- mean subtraction, 
	- 怎么做：Subtract per-channel mean and Divide by per-channel std
- normalization, 
- PCA/whitening
	- PCA: keeps dimensions which contain the most variance
	- whitening: takes the data into the eigenbasis and divides every dimension by its eigenvalue to normalize the scale

smaller networks can be preferred if the data is not complex enough to prevent overfitting[False]
- there are many **other preferred ways** to prevent overfitting: L2 regularization, 
- , input noise
- **problem** of controlling the number of neurons: smaller networks are harder to train with local methods such as Gradient Descent: It's clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss.

### Weight Initialization
**what we need?**
Activations are nicely scaled for all layers(不能让activations collapse to zero)

**strategy:**
"Xavier” Initialization

- Initialization too small:
Activations go to zero, gradients also zero, No learning =(
- Initialization too big:
Activations saturate (for tanh), Gradients zero, no learning =(
- Initialization just right:
Nice distribution of activations at all layers, Learning proceeds nicely =)

**breaking the symmetry?**
- what if initializing every neurons to have zero weight: then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates (in symmetry)
- what if adding random bias: it's sufficient to break symmetry. (as it can make the gradient different SEE PIAZZA TA's answer)

**The size of initial neuron**
- variance grows with the number of inputs
- batchnorm will have less noise
- recommended weight scale: scale by 1/sqrt(n), where n is the number of inputs

### Batch Normalization
activations：
- zero-mean 
- unit-variance 

需要区别：
- train：
- test：Estimates depend on minibatch; can’t do this at test-time!

> 如果本身数据很少的话不适合使用：minibatch只sample其中一部分来计算mean，var，所以数据太少的话这样操作只会有很大的noise

时机：
- Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.
- normalization is a simple differentiable operation, so is it possible to force activations throughout a network to take on a unit gaussian distribution at the beginning of training? (没看懂)
优缺点： 
- Makes deep networks much **easier to train**!（don't have to care for some details）
- Improves gradient flow
- Allows higher learning rates, **faster convergence**
- Networks become more robust to initialization
- Acts as **regularization** during training
- Zero overhead at test-time: can be fused with conv!（During testing batchnorm becomes a linear operator! Can be fused with the previous fully-connected or conv layer）我的理解就是linear层可以合并为一层
- **缺点：** Behaves differently during training and testing: this
is a very common source of bugs!

### Optimization
实际运用：
- **Adam** is a good default choice in many cases; it often works ok even with constant learning rate
- **SGD+Momentum** can outperform Adam but may require more tuning of LR and schedule
	- Try cosine schedule, very few hyperparameters!
- If you can afford to do full batch updates then try out **L-BFGS** (and don’t forget to disable all sources of noise)
	- it **must** be computed over entire training set

#### SGD
**缺点：** 三个角度：**Local Minima/Saddle points；conditioning；gradient noise**
- condition: loss changes quickly in one direction and slowly in another (当在不同方向梯度不同的时候)
What does gradient descent do? Very slow progress along shallow dimension, jitter along steep direction （每次沿着斜率大的方向update的比较多）
Loss function has **high condition number**: ratio of largest to smallest
singular value of the Hessian matrix is large
- loss function has a **local minima** or **saddle point**
Zero gradient, gradient descent gets stuck
- Our gradients come from minibatches so they can be noisy!

改进：SGD + Momentum

#### SGD + Momentum
solve the problem of local minima or saddle point

#### AdaGrad
Added element-wise **scaling** of the gradient based on the historical sum of squares in each dimension
“**Per-parameter learning rates**” or “adaptive learning rates”

优点：
- Progress along “steep” directions is damped;
progress along “flat” directions is accelerated
即缓解不同方向梯度不同造成的high conditioning
- Step size will decay to zero over long time(因为除数每次会加上dx^2)

改进：**RMSProp: “Leaky AdaGrad”**：引入新的参数decay_rate；in case learning rate decreases to 0

> adagrad: 
> cache += dx\**2
> x += - learning_rate * dx / (np.sqrt(cache) + eps)
> RMSProp: 
> cache = decay_rate * cache + (1 - decay_rate) * dx**2
> x += - learning_rate * dx / (np.sqrt(cache) + eps)
> unlike Adagrad, the gradient updates of RMSProp do not get exponentially smaller
#### Adam
引入两个参数beta1，beta2
在AdaGrad / RMSProp的基础上引入Momentum

问题：first and second momentum start at zero(我觉得大概可以理解为刚开始momentum 比较小)
解决：**Bias correction** for the fact that first and second moment estimates start at zero

#### L-BFGS
以上都是linear optimization
这个是second order的(quadratic)
**缺点是参数多**（Hessian has O(N^2) elements Inverting takes **O(N^3)**）

- **Usually works very well in full batch, deterministic mode**
i.e. if you have a single, deterministic f(x) then L-BFGS will probably work very nicely
- **Does not transfer very well to mini-batch setting.** Gives bad results. Adapting second-order methods to large-scale, stochastic setting is an active area of research.

### Learning rate schedules
- 是一种hyperparameter
- **learing rate 来控制loss的下降速度**
- All of them is useful!!! Start with large learning rate and decay over time；策略是慢慢的改变learning rate(一般是慢慢减小)
- **learning rate and batch size**(经验)：increase the batch size by N, also scale the initial learning rate by N
- High initial learning rates can make **loss explode**; linearly increasing learning rate from 0 over the first ~5000 iterations can prevent this


> loss and gradient exploding or vanishing???

### Regularization
- L1, L2, Elastic net (L1 + L2)
- dropout

#### Dropout
**The idea of using dropout**
- keeps a neuron active with probability p
- **Forces the network to have a redundant representation(use more bits than needed to represent); Prevents co-adaptation of features**
- Dropout is training a large **ensemble of models** (that share parameters). Each binary mask is one model

**注意两点**
- dropout at forward time
- scale at test time

解释：
- In each forward pass, randomly set some neurons to zero；Probability of dropping is a hyperparameter; 0.5 is common
- At test time all neurons are active always => We must scale the activations so that for each neuron: output at test time = expected output at training time （train 的是时候dropout了，数据变小了。test则是所有点都参与，为了test可以和train的数据形状一样，在test activation的时候乘一个scale = p）

**Data Augmentation**
- rotate, stretching... 
- Randomize contrast and brightness

Training: Add random noise
Testing: Marginalize over the noise

- Consider dropout for large fully-connected layers
- Batch normalization and data augmentation almost always a good idea
- Try cutout and mixup especially for small classification datasets
### Pooling layer
progressively **reduce the spatial size** of the representation to **reduce the amount of parameters** and computation in the network, and hence to also control overfitting

# CNN Architecture
重点：
- 计算output的形状
- 计算parameter的数量

常见CNN
- AlexNet
- VGG
- GoogLeNet
- ResNet

### VGGNet 和 AlexNet的区别
VGGNet uses small filters: （3个3\*3conv layer instead of 1 个7\*7 conv layer）
- has the same **receptive field** (重点：RF的理解和计算)
- deeper, more non-linearities
- fewer parameters

### GoogLeNet
**“Inception module”:** design a good local network topology (network within a network) and then stack these modules on top of each other （我的理解就是parallel filter重复使用，然后Concatenate all filter outputs together depth-wise）

缺点：**Very expensive compute**
解决方法：**"bottlenect layer"**： using 1*1 conv layer to project depth on lower dimension; **preserve spatial but reduce depth**

### ResNet
效果突飞猛进
Very deep networks using **residual connections**

**residual connections**: 
Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping
我的理解就是use learned layers
# RNN Architecture
**Key idea:** 
- RNNs have an “**internal state**” that is **updated as a sequence is processed**
- We can process a sequence of vectors x by applying a recurrence formula at every time step（就是同一个W对a sequence of x(或1个)使用，再把得到的结果组合起来）
- encode and decode：many to one + one to many （常用nlp）

很多细节没看懂

**Modified Version of RNN:**
- LSTM(long short term memory)
- GRU

**Gradients vanishing and exploding problems in RNN:**
- too big: **Gradient clipping**: Scale Computing gradient gradient if its norm is too big (就是除以它的norm)
- too small: change RNN architecture (compute intermediate values: **gates in LSTM**)

### LSTM
Common to use LSTM or GRU: their additive interactions improve gradient flow
Backpropagation from ct to ct-1 only elementwise multiplication by f, no matrix multiply by W

- i: Input gate, whether to write to cell
- f: Forget gate, Whether to erase cell
- o: Output gate, How much to reveal cell
- g: Gate gate (?), How much to write to cell

